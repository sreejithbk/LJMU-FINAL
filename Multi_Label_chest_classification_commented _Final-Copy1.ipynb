{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a4d89dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-14 14:26:36.463876: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications import DenseNet121\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import Conv2D, Reshape, Activation, Multiply, Lambda\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b0d4cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-14 14:26:40.815917: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-14 14:26:40.826900: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-14 14:26:40.827138: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-14 14:26:40.828848: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-14 14:26:40.829085: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-14 14:26:40.829291: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-14 14:26:42.153937: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-14 14:26:42.154117: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-14 14:26:42.154250: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-14 14:26:42.154361: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22078 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:53:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-14 14:26:44.425092: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2023-06-14 14:26:57.287888: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"NVIDIA GeForce RTX 4090\" frequency: 2520 num_cores: 128 environment { key: \"architecture\" value: \"8.9\" } environment { key: \"cuda\" value: \"11080\" } environment { key: \"cudnn\" value: \"8600\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 75497472 shared_memory_size_per_multiprocessor: 102400 memory_size: 23151050752 bandwidth: 1008096000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "2023-06-14 14:26:59.583428: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8600\n",
      "2023-06-14 14:27:00.931657: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:637] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-06-14 14:27:00.946308: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x222d5e50 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-06-14 14:27:00.946337: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA GeForce RTX 4090, Compute Capability 8.9\n",
      "2023-06-14 14:27:00.952325: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-06-14 14:27:01.065455: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2330/2330 [==============================] - ETA: 0s - loss: 0.2922 - accuracy: 0.2896 - auc: 0.7895"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-14 14:33:08.382817: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_auc improved from -inf to 0.80475, saving model to best_model_self_attention_dense.h5\n",
      "2330/2330 [==============================] - 425s 165ms/step - loss: 0.2922 - accuracy: 0.2896 - auc: 0.7895 - val_loss: 0.2882 - val_accuracy: 0.3308 - val_auc: 0.8048\n",
      "Epoch 2/500\n",
      "2330/2330 [==============================] - ETA: 0s - loss: 0.2772 - accuracy: 0.3302 - auc: 0.8199\n",
      "Epoch 2: val_auc did not improve from 0.80475\n",
      "2330/2330 [==============================] - 377s 162ms/step - loss: 0.2772 - accuracy: 0.3302 - auc: 0.8199 - val_loss: 0.2952 - val_accuracy: 0.2869 - val_auc: 0.7977\n",
      "Epoch 3/500\n",
      "2330/2330 [==============================] - ETA: 0s - loss: 0.2696 - accuracy: 0.3458 - auc: 0.8335\n",
      "Epoch 3: val_auc did not improve from 0.80475\n",
      "2330/2330 [==============================] - 372s 160ms/step - loss: 0.2696 - accuracy: 0.3458 - auc: 0.8335 - val_loss: 0.2900 - val_accuracy: 0.3086 - val_auc: 0.8039\n",
      "Epoch 4/500\n",
      "2330/2330 [==============================] - ETA: 0s - loss: 0.2639 - accuracy: 0.3613 - auc: 0.8425\n",
      "Epoch 4: val_auc improved from 0.80475 to 0.82951, saving model to best_model_self_attention_dense.h5\n",
      "2330/2330 [==============================] - 381s 164ms/step - loss: 0.2639 - accuracy: 0.3613 - auc: 0.8425 - val_loss: 0.2728 - val_accuracy: 0.3482 - val_auc: 0.8295\n",
      "Epoch 5/500\n",
      "2330/2330 [==============================] - ETA: 0s - loss: 0.2600 - accuracy: 0.3710 - auc: 0.8486\n",
      "Epoch 5: val_auc improved from 0.82951 to 0.83200, saving model to best_model_self_attention_dense.h5\n",
      "2330/2330 [==============================] - 380s 163ms/step - loss: 0.2600 - accuracy: 0.3710 - auc: 0.8486 - val_loss: 0.2753 - val_accuracy: 0.3531 - val_auc: 0.8320\n",
      "Epoch 6/500\n",
      "2330/2330 [==============================] - ETA: 0s - loss: 0.2564 - accuracy: 0.3799 - auc: 0.8542\n",
      "Epoch 6: val_auc did not improve from 0.83200\n",
      "2330/2330 [==============================] - 378s 162ms/step - loss: 0.2564 - accuracy: 0.3799 - auc: 0.8542 - val_loss: 0.2849 - val_accuracy: 0.3226 - val_auc: 0.8227\n",
      "Epoch 7/500\n",
      "2330/2330 [==============================] - ETA: 0s - loss: 0.2524 - accuracy: 0.3848 - auc: 0.8600\n",
      "Epoch 7: val_auc did not improve from 0.83200\n",
      "2330/2330 [==============================] - 373s 160ms/step - loss: 0.2524 - accuracy: 0.3848 - auc: 0.8600 - val_loss: 0.2713 - val_accuracy: 0.3376 - val_auc: 0.8320\n",
      "Epoch 8/500\n",
      "2330/2330 [==============================] - ETA: 0s - loss: 0.2483 - accuracy: 0.3944 - auc: 0.8660\n",
      "Epoch 8: val_auc improved from 0.83200 to 0.83414, saving model to best_model_self_attention_dense.h5\n",
      "2330/2330 [==============================] - 373s 160ms/step - loss: 0.2483 - accuracy: 0.3944 - auc: 0.8660 - val_loss: 0.2738 - val_accuracy: 0.3675 - val_auc: 0.8341\n",
      "Epoch 9/500\n",
      "2330/2330 [==============================] - ETA: 0s - loss: 0.2437 - accuracy: 0.4079 - auc: 0.8719\n",
      "Epoch 9: val_auc did not improve from 0.83414\n",
      "2330/2330 [==============================] - 378s 162ms/step - loss: 0.2437 - accuracy: 0.4079 - auc: 0.8719 - val_loss: 0.2783 - val_accuracy: 0.3840 - val_auc: 0.8335\n",
      "Epoch 10/500\n",
      "2330/2330 [==============================] - ETA: 0s - loss: 0.2385 - accuracy: 0.4182 - auc: 0.8786\n",
      "Epoch 10: val_auc did not improve from 0.83414\n",
      "2330/2330 [==============================] - 378s 162ms/step - loss: 0.2385 - accuracy: 0.4182 - auc: 0.8786 - val_loss: 0.2798 - val_accuracy: 0.3685 - val_auc: 0.8241\n",
      "Epoch 11/500\n",
      "2330/2330 [==============================] - ETA: 0s - loss: 0.2327 - accuracy: 0.4308 - auc: 0.8859\n",
      "Epoch 11: val_auc improved from 0.83414 to 0.83788, saving model to best_model_self_attention_dense.h5\n",
      "2330/2330 [==============================] - 371s 159ms/step - loss: 0.2327 - accuracy: 0.4308 - auc: 0.8859 - val_loss: 0.2704 - val_accuracy: 0.3634 - val_auc: 0.8379\n",
      "Epoch 12/500\n",
      "2330/2330 [==============================] - ETA: 0s - loss: 0.2258 - accuracy: 0.4492 - auc: 0.8939\n",
      "Epoch 12: val_auc improved from 0.83788 to 0.83982, saving model to best_model_self_attention_dense.h5\n",
      "2330/2330 [==============================] - 379s 163ms/step - loss: 0.2258 - accuracy: 0.4492 - auc: 0.8939 - val_loss: 0.2833 - val_accuracy: 0.3675 - val_auc: 0.8398\n",
      "Epoch 13/500\n",
      "2330/2330 [==============================] - ETA: 0s - loss: 0.2175 - accuracy: 0.4682 - auc: 0.9029\n",
      "Epoch 13: val_auc did not improve from 0.83982\n",
      "2330/2330 [==============================] - 378s 162ms/step - loss: 0.2175 - accuracy: 0.4682 - auc: 0.9029 - val_loss: 0.2918 - val_accuracy: 0.3789 - val_auc: 0.8326\n",
      "Epoch 14/500\n",
      "2330/2330 [==============================] - ETA: 0s - loss: 0.2076 - accuracy: 0.4902 - auc: 0.9131\n",
      "Epoch 14: val_auc did not improve from 0.83982\n",
      "2330/2330 [==============================] - 372s 160ms/step - loss: 0.2076 - accuracy: 0.4902 - auc: 0.9131 - val_loss: 0.2918 - val_accuracy: 0.3613 - val_auc: 0.8327\n",
      "Epoch 15/500\n",
      "2330/2330 [==============================] - ETA: 0s - loss: 0.1958 - accuracy: 0.5183 - auc: 0.9236\n",
      "Epoch 15: val_auc did not improve from 0.83982\n",
      "2330/2330 [==============================] - 374s 160ms/step - loss: 0.1958 - accuracy: 0.5183 - auc: 0.9236 - val_loss: 0.3082 - val_accuracy: 0.3308 - val_auc: 0.8124\n",
      "Epoch 16/500\n",
      "2330/2330 [==============================] - ETA: 0s - loss: 0.1837 - accuracy: 0.5434 - auc: 0.9336\n",
      "Epoch 16: val_auc did not improve from 0.83982\n",
      "2330/2330 [==============================] - 376s 161ms/step - loss: 0.1837 - accuracy: 0.5434 - auc: 0.9336 - val_loss: 0.3205 - val_accuracy: 0.3579 - val_auc: 0.8269\n",
      "Epoch 17/500\n",
      "2330/2330 [==============================] - ETA: 0s - loss: 0.1711 - accuracy: 0.5729 - auc: 0.9432\n",
      "Epoch 17: val_auc did not improve from 0.83982\n",
      "2330/2330 [==============================] - 376s 161ms/step - loss: 0.1711 - accuracy: 0.5729 - auc: 0.9432 - val_loss: 0.3274 - val_accuracy: 0.3270 - val_auc: 0.8184\n",
      "Epoch 18/500\n",
      "2330/2330 [==============================] - ETA: 0s - loss: 0.1584 - accuracy: 0.6043 - auc: 0.9519\n",
      "Epoch 18: val_auc did not improve from 0.83982\n",
      "2330/2330 [==============================] - 378s 162ms/step - loss: 0.1584 - accuracy: 0.6043 - auc: 0.9519 - val_loss: 0.3368 - val_accuracy: 0.3168 - val_auc: 0.8099\n",
      "Epoch 19/500\n",
      " 732/2330 [========>.....................] - ETA: 4:00 - loss: 0.1400 - accuracy: 0.6430 - auc: 0.9633"
     ]
    }
   ],
   "source": [
    "class LogWriterCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, log_path):\n",
    "        super(LogWriterCallback, self).__init__()\n",
    "        self.log_path = log_path\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        # This method is called at the beginning of the training.\n",
    "        # It opens the log file and writes the starting time of the training.\n",
    "        with open(self.log_path, 'a') as file:\n",
    "            file.write('Training started at {}\\n'.format(datetime.datetime.now()))\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # This method is called at the end of each training epoch.\n",
    "        # It opens the log file and writes the epoch number and the values in the logs.\n",
    "        with open(self.log_path, 'a') as file:\n",
    "            file.write('Epoch {}\\n'.format(epoch + 1))\n",
    "            for key, value in logs.items():\n",
    "                file.write('{}: {}\\n'.format(key, value))\n",
    "            file.write('\\n')\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        # This method is called at the end of the training.\n",
    "        # It opens the log file and writes the finishing time of the training.\n",
    "        with open(self.log_path, 'a') as file:\n",
    "            file.write('Training finished at {}\\n'.format(datetime.datetime.now()))\n",
    "\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, image_paths, labels, batch_size, image_size):\n",
    "        # Initialize the DataGenerator with the provided image paths, labels, batch size, and image size.\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.image_size = image_size\n",
    "\n",
    "    def __len__(self):\n",
    "        # Returns the number of batches in the dataset.\n",
    "        return int(np.ceil(len(self.image_paths) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Generates one batch of data based on the index.\n",
    "        # Get the paths and labels for the current batch.\n",
    "        batch_paths = self.image_paths[index * self.batch_size: (index + 1) * self.batch_size]\n",
    "        batch_labels = self.labels[index * self.batch_size: (index + 1) * self.batch_size]\n",
    "\n",
    "        # Preprocess the images for the current batch.\n",
    "        batch_images = [self.preprocess_image(image_path) for image_path in batch_paths]\n",
    "        batch_images = np.array(batch_images)\n",
    "\n",
    "        # Return the preprocessed batch of images and corresponding labels.\n",
    "        return batch_images, batch_labels\n",
    "\n",
    "    def preprocess_image(self, image_path):\n",
    "        # Preprocesses a single image given its path.\n",
    "        # Load the image and resize it to the specified image size.\n",
    "        image = keras.preprocessing.image.load_img(image_path, target_size=self.image_size)\n",
    "\n",
    "        # Convert the image to a numpy array.\n",
    "        image = keras.preprocessing.image.img_to_array(image)\n",
    "\n",
    "        # Normalize the pixel values to the range [0, 1].\n",
    "        image = image / 255.0\n",
    "\n",
    "        # Return the preprocessed image.\n",
    "        return image\n",
    "\n",
    "\n",
    "def load_dataset(data_set_path, data_csv_path):\n",
    "    # Load the data from the CSV file using pandas read_csv function.\n",
    "    data_df = pd.read_csv(data_csv_path)\n",
    "\n",
    "    # Remove rows where the \"Finding Labels\" column has the value 'No Finding',\n",
    "    # as they represent cases with no findings and are not relevant for this dataset.\n",
    "    data_df = data_df[data_df[\"Finding Labels\"] != 'No Finding'].reset_index(drop=True)\n",
    "\n",
    "    # Create a list of image paths by joining the data set path with the \"Image Index\" column in the DataFrame.\n",
    "    image_paths = [os.path.join(data_set_path, row['Image Index']) for _, row in data_df[['Image Index']].iterrows()]\n",
    "\n",
    "    # Split the \"Finding Labels\" column by '|' to obtain a list of labels for each image.\n",
    "    labels = data_df[\"Finding Labels\"].apply(lambda x: x.split(\"|\"))\n",
    "\n",
    "    # Return the image paths and labels.\n",
    "    return image_paths, labels\n",
    "\n",
    "\n",
    "def preprocess_labels(labels, mlb=None):\n",
    "    # Preprocesses the labels using MultiLabelBinarizer.\n",
    "    # If mlb is not provided, a new MultiLabelBinarizer instance is created and fitted on the labels.\n",
    "    if mlb is None:\n",
    "        mlb = MultiLabelBinarizer()\n",
    "        labels = mlb.fit_transform(labels)\n",
    "    else:\n",
    "        # If mlb is provided, transform the labels using the existing MultiLabelBinarizer instance.\n",
    "        labels = mlb.transform(labels)\n",
    "\n",
    "    # Get the number of unique labels.\n",
    "    num_labels = len(mlb.classes_)\n",
    "\n",
    "    # Reshape the labels to have shape (-1, num_labels), where -1 means the size is inferred based on the data.\n",
    "    labels = np.reshape(labels, (-1, num_labels))\n",
    "\n",
    "    # Return the preprocessed labels, the MultiLabelBinarizer instance, and the number of labels.\n",
    "    return labels, mlb, num_labels\n",
    "\n",
    "\n",
    "def build_model_Res(input_shape, num_labels):\n",
    "    # Build a model using the ResNet50 architecture.\n",
    "\n",
    "    # Load the pre-trained ResNet50 model with weights from 'imagenet'.\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "\n",
    "    # Get the output tensor of the base model.\n",
    "    x = base_model.output\n",
    "\n",
    "    # Add a GlobalAveragePooling2D layer to reduce spatial dimensions.\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "    # Add a fully connected Dense layer with 1024 units and ReLU activation.\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "\n",
    "    # Add a final Dense layer with num_labels units and sigmoid activation for multi-label classification.\n",
    "    predictions = Dense(num_labels, activation='sigmoid')(x)\n",
    "\n",
    "    # Create the model by specifying the inputs and outputs.\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "    # Return the built model.\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_model_with_attention_Res(input_shape, num_labels, attention_units=256):\n",
    "    # Builds a model based on ResNet50 architecture with attention mechanism.\n",
    "\n",
    "    # Load the pre-trained ResNet50 model with weights trained on ImageNet.\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "\n",
    "    # Get the output tensor from the base model.\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "    # Attention mechanism\n",
    "    attention = Dense(attention_units, activation='tanh')(x)\n",
    "    attention = Dense(1, activation='sigmoid')(attention)\n",
    "    attention = Reshape((-1,))(attention)\n",
    "    attention = Multiply()([x, attention])\n",
    "    attention = Reshape((1, 1, -1))(attention)\n",
    "    attention = GlobalAveragePooling2D()(attention)\n",
    "\n",
    "    # Fully connected layers for classification\n",
    "    x = Dense(1024, activation='relu')(attention)\n",
    "    predictions = Dense(num_labels, activation='sigmoid')(x)\n",
    "\n",
    "    # Create the model with base_model.input as the input and predictions as the output.\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "    # Return the model\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_model_with_self_attention_Res(input_shape, num_labels, attention_units=256):\n",
    "    # Builds a model based on ResNet50 architecture with self attention mechanism.\n",
    "\n",
    "    # Load the pre-trained ResNet50 model with weights trained on ImageNet.\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    # Get the output tensor from the base model.\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "    # Self-Attention mechanism\n",
    "    attention = Reshape((1, 1, -1))(x)\n",
    "    attention = Conv2D(filters=attention_units, kernel_size=1, padding='same', activation='relu')(attention)\n",
    "    attention = Conv2D(filters=1, kernel_size=1, padding='same', activation='sigmoid')(attention)\n",
    "    attention = Lambda(lambda x: tf.reduce_mean(x, axis=[1, 2]))(attention)\n",
    "    attention = Activation('softmax')(attention)\n",
    "    attention = Reshape((-1, 1))(attention)\n",
    "    attention = Multiply()([x, attention])\n",
    "    attention = Lambda(lambda x: tf.reduce_mean(x, axis=1))(attention)\n",
    "    # Fully connected layers for classification\n",
    "    x = Dense(1024, activation='relu')(attention)\n",
    "    predictions = Dense(num_labels, activation='sigmoid')(x)\n",
    "    # Create the model with base_model.input as the input and predictions as the output.\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def build_model_with_self_attention_Dense(input_shape, num_labels, attention_units=256):\n",
    "    # Builds a model based on DenseNet150 architecture with self attention mechanism.\n",
    "\n",
    "    # Load the pre-trained DenseNet150 model with weights trained on ImageNet.\n",
    "    base_model = DenseNet121(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    # Get the output tensor from the base model.\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "    # Self-Attention mechanism\n",
    "    attention = Reshape((1, 1, -1))(x)\n",
    "    attention = Conv2D(filters=attention_units, kernel_size=1, padding='same', activation='relu')(attention)\n",
    "    attention = Conv2D(filters=1, kernel_size=1, padding='same', activation='sigmoid')(attention)\n",
    "    attention = Lambda(lambda x: tf.reduce_mean(x, axis=[1, 2]))(attention)\n",
    "    attention = Activation('softmax')(attention)\n",
    "    attention = Reshape((-1, 1))(attention)\n",
    "    attention = Multiply()([x, attention])\n",
    "    attention = Lambda(lambda x: tf.reduce_mean(x, axis=1))(attention)\n",
    "    # Fully connected layers for classification\n",
    "    x = Dense(1024, activation='relu')(attention)\n",
    "    predictions = Dense(num_labels, activation='sigmoid')(x)\n",
    "    # Create the model with base_model.input as the input and predictions as the output.\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    return model\n",
    "\n",
    "def build_model_with_categorical_self_attention_Dense(input_shape, num_labels, attention_units=256):\n",
    "    # Builds a model based on DenseNet150 architecture with categorical wise self attention mechanism.\n",
    "\n",
    "    # Load the pre-trained DenseNet150 model with weights trained on ImageNet.\n",
    "    base_model = DenseNet121(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    # Get the output tensor from the base model.\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "    # Categorical-wise Self-Attention mechanism\n",
    "    num_categories = int(x.shape[-1])\n",
    "    attention = Reshape((1, 1, num_categories))(x)\n",
    "    attention = Conv2D(filters=attention_units, kernel_size=1, padding='same', activation='relu')(attention)\n",
    "    attention = Conv2D(filters=num_categories, kernel_size=1, padding='same', activation='sigmoid')(attention)\n",
    "    attention = Lambda(lambda x: tf.reduce_mean(x, axis=[1, 2]))(attention)\n",
    "    attention = Activation('softmax')(attention)\n",
    "    attention = Reshape((-1, 1))(attention)\n",
    "    attention = Multiply()([x, attention])\n",
    "    attention = Lambda(lambda x: tf.reduce_mean(x, axis=1))(attention)\n",
    "    # Fully connected layers for classification\n",
    "    x = Dense(1024, activation='relu')(attention)\n",
    "    predictions = Dense(num_labels, activation='sigmoid')(x)\n",
    "    # Create the model with base_model.input as the input and predictions as the output.\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_model(model, train_generator, val_generator, epochs, callbacks):\n",
    "\n",
    "    # This line compiles the model with the Adam optimizer, binary crossentropy loss, and accuracy and AUC metrics.\n",
    "    \"\"\"\n",
    "    Compiles the model with the specified optimizer, loss, and metrics.\n",
    "\n",
    "    Args:\n",
    "        model: The model to compile.\n",
    "        optimizer: The optimizer to use.\n",
    "        loss: The loss function to minimize.\n",
    "        metrics: The metrics to monitor during training and evaluation.\n",
    "\n",
    "    Returns:\n",
    "        The compiled model.\n",
    "    \"\"\"\n",
    "\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC()])\n",
    "\n",
    "    # This line fits the model to the training data, using the validation data for validation. The epochs parameter specifies the number of times to train the model on the training data. The callbacks parameter specifies a list of callbacks that will be called during training.\n",
    "    \"\"\"\n",
    "    Fits the model to the training data, using the validation data for validation.\n",
    "\n",
    "    Args:\n",
    "        model: The model to fit.\n",
    "        train_generator: The training data generator.\n",
    "        val_generator: The validation data generator.\n",
    "        epochs: The number of epochs to train the model.\n",
    "        callbacks: A list of callbacks to be called during training.\n",
    "\n",
    "    Returns:\n",
    "        The history of the model training.\n",
    "    \"\"\"\n",
    "\n",
    "    model.fit(train_generator, validation_data=val_generator, epochs=epochs, callbacks=callbacks)\n",
    "\n",
    "def evaluate_model(model, test_generator):\n",
    "\n",
    "    # This line evaluates the model on the test data and prints the loss, accuracy, and AUC metrics.\n",
    "\n",
    "    \"\"\"\n",
    "    Evaluates the model on the test data and prints the loss, accuracy, and AUC metrics.\n",
    "\n",
    "    Args:\n",
    "        model: The model to evaluate.\n",
    "        test_generator: The test data generator.\n",
    "\n",
    "    Returns:\n",
    "        The loss, accuracy, and AUC metrics.\n",
    "    \"\"\"\n",
    "\n",
    "    loss, accuracy, auc = model.evaluate(test_generator)\n",
    "    print(\"Test Loss:\", loss)\n",
    "    print(\"Test Accuracy:\", accuracy)\n",
    "    print(\"Test AUC:\", auc)\n",
    "\n",
    "\n",
    "# Set the paths and parameters\n",
    "data_set_path = \"./datasets/NIH/images/\"\n",
    "dataset_folder = os.path.join(\"./\", \"nih_dataset\")\n",
    "data_csv_path = os.path.join(dataset_folder, \"Data_Entry_2017.csv\")\n",
    "batch_size = 16\n",
    "image_size = (224, 224)\n",
    "\n",
    "# Load the dataset metadata\n",
    "image_paths, labels = load_dataset(data_set_path, data_csv_path)\n",
    "\n",
    "# Perform train-validation-test split\n",
    "train_paths, test_paths, train_labels, test_labels = train_test_split(image_paths, labels, test_size=0.2, random_state=42)\n",
    "train_paths, val_paths, train_labels, val_labels = train_test_split(train_paths, train_labels, test_size=0.1, random_state=42)\n",
    "\n",
    "# Convert labels to binary vectors\n",
    "train_labels, mlb, num_labels = preprocess_labels(train_labels)\n",
    "val_labels, _, _ = preprocess_labels(val_labels, mlb)\n",
    "test_labels, _, _ = preprocess_labels(test_labels, mlb)\n",
    "\n",
    "# Create data generators\n",
    "train_generator = DataGenerator(train_paths, train_labels, batch_size, image_size)\n",
    "val_generator = DataGenerator(val_paths, val_labels, batch_size, image_size)\n",
    "test_generator = DataGenerator(test_paths, test_labels, batch_size, image_size)\n",
    "\n",
    "# Build the model\n",
    "\n",
    "#model = build_model_Res(input_shape=(224, 224, 3), num_labels=num_labels)\n",
    "#model = build_model_with_attention_Res(input_shape=(224, 224, 3), num_labels=num_labels)\n",
    "#model = build_model_with_self_attention_Res(input_shape=(224, 224, 3), num_labels=num_labels)\n",
    "\n",
    "model = build_model_with_self_attention_Dense(input_shape=(224, 224, 3), num_labels=num_labels)\n",
    "#model =  build_model_with_categorical_self_attention_Dense(input_shape=(224, 224, 3), num_labels=num_labels)\n",
    "\n",
    "# Define the early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_auc', patience=10, mode='max', verbose=1)\n",
    "\n",
    "# Define the log path\n",
    "approach_name = \"self_attention_dense\"\n",
    "\n",
    "log_path = 'training_{}_log.txt'.format(approach_name)\n",
    "# Define the log writer callback\n",
    "log_writer = LogWriterCallback(log_path)\n",
    "\n",
    "# Define the checkpoint callback\n",
    "checkpoint_path = 'best_model_{}.h5'.format(approach_name)\n",
    "checkpoint = ModelCheckpoint(checkpoint_path, monitor='val_auc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks = [checkpoint,early_stopping,log_writer]\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_generator, val_generator, epochs= 500, callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8991b76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "best_model = keras.models.load_model(checkpoint_path)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "evaluate_model(best_model, test_generator)\n",
    "\n",
    "# Calculate the average AUC on the test set\n",
    "test_pred_prob = best_model.predict(test_generator)\n",
    "test_true_labels = mlb.transform(test_labels)\n",
    "auc_scores = []\n",
    "for i in range(num_labels):\n",
    "    auc = roc_auc_score(test_true_labels[:, i], test_pred_prob[:, i])\n",
    "    auc_scores.append(auc)\n",
    "average_auc = np.mean(auc_scores)\n",
    "print(\"Average AUC:\", average_auc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
